{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Regression Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_TIME_COLUMNS = ['DATETIME_INI', 'DATETIME_FIM']\n",
    "\n",
    "ds = pd.read_csv('../../../data/preprocessed.csv', index_col='PES_ID', parse_dates=DATE_TIME_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"DATETIME_INI\"] = ds[\"DATETIME_INI\"].apply(lambda x: time.mktime(x.timetuple()))\n",
    "ds[\"DATETIME_FIM\"] = ds[\"DATETIME_FIM\"].apply(lambda x: time.mktime(x.timetuple()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation coefficients for 'PES_PESOUTIL' with the other variables. The closer they are to 1, the more correlation there is.\n",
    "# Very high correlations mean that you can expect a straightforward modeling process.\n",
    "# Source: Real Python (https://realpython.com/knn-python/#a-step-by-step-knn-from-scratch-in-python)\n",
    "correlation_matrix = ds.corr()\n",
    "correlation_matrix['PES_PESOUTIL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column_name = 'PES_PESOUTIL'\n",
    "target_column = ds.pop(target_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ds.to_numpy()\n",
    "y = target_column.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameterizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# # set up our search grid\n",
    "# param_grid = {\"algorithm\": ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "#               \"p\": [1, 2],\n",
    "#               \"leaf_size\": [10, 30, 50],\n",
    "#               \"n_neighbors\": [1, 10, 100, 1000],\n",
    "#               \"weights\": [\"uniform\", \"distance\"]\n",
    "# }\n",
    "\n",
    "# knn_model = KNeighborsRegressor()\n",
    "\n",
    "# # try out every combination of the above values\n",
    "# search = GridSearchCV(knn_model, param_grid, cv=10).fit(X_train, y_train)\n",
    "\n",
    "# print(\"The best hyperparameters are \",search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'algorithm': 'ball_tree',\n",
    "          'leaf_size': 30,\n",
    "          'n_neighbors': 100,\n",
    "          'weights': 'distance',\n",
    "          'p': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(**params)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = knn.predict(X_train)\n",
    "train_mse = mean_squared_error(y_train, train_preds)\n",
    "train_rmse = math.sqrt(train_mse)\n",
    "train_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = knn.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, test_preds)\n",
    "test_rmse = math.sqrt(test_mse)\n",
    "test_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "def cross_validate_model(model, X, y, cv=10):\n",
    "    cv = ShuffleSplit(n_splits=cv, test_size=0.3, random_state=random.randint(0, 1000))\n",
    "    n_scores = cross_validate(estimator=model,\n",
    "                            X=X,\n",
    "                            y=y,\n",
    "                            cv=cv,\n",
    "                            scoring=('r2', 'neg_mean_squared_error', 'neg_root_mean_squared_error'),\n",
    "                            return_train_score=True\n",
    "                            )\n",
    "    return n_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITERATIONS = 5\n",
    "N_CROSS_VALIDATION = 10\n",
    "\n",
    "n_scores = []\n",
    "duration = []\n",
    "\n",
    "for _ in range(N_ITERATIONS):\n",
    "    time_start = time.time()\n",
    "    n_scores.append(cross_validate_model(knn, X, y, cv=N_CROSS_VALIDATION))\n",
    "    duration.append(time.time() - time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(n_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot\n",
    "plt.bar(range(1,len(duration)+1), duration)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Duration (seconds)')\n",
    "plt.title('Bar Plot of Duration for Each Iteration')\n",
    "plt.show()\n",
    "\n",
    "time_training = np.mean(duration)\n",
    "time_training_std = np.std(duration)\n",
    "\n",
    "print(f\"Training time: {time_training} ± {time_training_std} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R² Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the R2 scores\n",
    "r2_scores = [r2['test_r2'].reshape(-1, 1) for r2 in n_scores]\n",
    "\n",
    "# Plot the R2 scores\n",
    "[plt.plot(r2_score, label=f'Fold {i + 1}') for i, r2_score in enumerate(r2_scores)]\n",
    "plt.xlabel('Cross-validation fold')\n",
    "plt.ylabel('R2 score')\n",
    "plt.title('R2 Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation of R2 scores\n",
    "mean_r2_scores = [r2['test_r2'].mean() for r2 in n_scores]\n",
    "std_r2_scores = [r2['test_r2'].std() for r2 in n_scores]\n",
    "\n",
    "# Plot the mean R2 scores with error bars representing the standard deviation\n",
    "plt.errorbar(range(1, N_ITERATIONS + 1), mean_r2_scores, yerr=std_r2_scores, marker='o', linestyle='-', capsize=5)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('R2 score')\n",
    "plt.title('Mean R2 Scores with Standard Deviation')\n",
    "plt.show()\n",
    "\n",
    "mean_r2 = sum(mean_r2_scores) / len(mean_r2_scores)\n",
    "std_mse = np.std(mean_r2_scores)\n",
    "\n",
    "print(\"Mean R2: {:.4f}\".format(mean_r2))\n",
    "print(\"Standard Deviation of R2: {:.4f}\".format(std_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE for each fold\n",
    "mse_scores = [-r2['test_neg_mean_squared_error'].mean() for r2 in n_scores]\n",
    "\n",
    "# Plot the MSE scores\n",
    "plt.plot(range(1, N_ITERATIONS + 1), mse_scores, marker='o', linestyle='-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE Scores')\n",
    "plt.show()\n",
    "\n",
    "# Calculate mean and standard deviation of MSE scores\n",
    "mean_mse = sum(mse_scores) / len(mse_scores)\n",
    "std_mse = np.std(mse_scores)\n",
    "\n",
    "print(\"Mean MSE: {:.4f}\".format(mean_mse))\n",
    "print(\"Standard Deviation of MSE: {:.4f}\".format(std_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_scores = [np.sqrt(-r2['test_neg_mean_squared_error'].mean()) for r2 in n_scores]\n",
    "\n",
    "# Plot the RMSE scores\n",
    "plt.plot(range(1, N_ITERATIONS + 1), rmse_scores, marker='o', linestyle='-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE Scores')\n",
    "plt.show()\n",
    "\n",
    "# Calculate mean and standard deviation of RMSE scores\n",
    "mean_rmse = sum(rmse_scores) / len(rmse_scores)\n",
    "std_rmse = np.std(rmse_scores)\n",
    "\n",
    "print(\"Mean RMSE: {:.4f}\".format(mean_rmse))\n",
    "print(\"Standard Deviation of RMSE: {:.4f}\".format(std_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the filename for the metrics dataset\n",
    "metrics_filename = \"../results.csv\"\n",
    "\n",
    "# Check if the metrics.csv file exists\n",
    "if os.path.exists(metrics_filename):\n",
    "    # Load the metrics dataset\n",
    "    metrics_dataset = pd.read_csv(metrics_filename, index_col='Algorithm')\n",
    "    \n",
    "    data = {\n",
    "        \"Mean R2 score\": mean_r2,\n",
    "        \"Standard deviation of R2 scores\": std_mse,\n",
    "        \"Mean MSE\": mean_mse,\n",
    "        \"Standard deviation of MSE\": std_mse,\n",
    "        \"Mean RMSE\": mean_rmse,\n",
    "        \"Standard deviation of RMSE\": std_rmse,\n",
    "        \"Mean Training time\": time_training,\n",
    "        \"Standard deviation of Training time\": time_training_std\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data, index=['KNNRegressor_preprocessed'])\n",
    "    \n",
    "    metrics_dataset = pd.concat([metrics_dataset, df], axis=0)\n",
    "    \n",
    "    # Save the updated metrics dataset\n",
    "    metrics_dataset.to_csv(metrics_filename, index=True, index_label='Algorithm')\n",
    "else:\n",
    "    # The metrics.csv file does not exist, print an error message\n",
    "    print(\"The metrics.csv file does not exist.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
